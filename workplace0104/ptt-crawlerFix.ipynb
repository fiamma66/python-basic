{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare packages\n",
    "import requests, pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "# from pythonjsonlogger import jsonlogger\n",
    "import logmatic\n",
    "import os\n",
    "import time, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 設定現在時間 以時間來作為log檔名 一個log 以日計算\n",
    "# 並在當下目錄創建 log folder\n",
    "def get_time():\n",
    "    now = datetime.utcnow()\n",
    "    path = \"./log\"\n",
    "\n",
    "    utc_8 = timezone(timedelta(hours=8))\n",
    "    now = now.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    time_now = now.astimezone(utc_8)\n",
    "\n",
    "    split = str(time_now).split(\" \")\n",
    "    file_date = split[0]\n",
    "    split = split[1].split(\":\")\n",
    "    file_time = split[0]+split[1]\n",
    "    if not os.path.exists(path) :\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    return path + \"/\" + file_date \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory\n",
    "#url = \"https://www.ptt.cc/bbs/Food/index2.html\"\n",
    "\n",
    "# set proxy to use\n",
    "proxylist = [{\"http\": \"37.187.120.123:80\"},\n",
    "             {\"https\": \"206.189.36.198:8080\"},\n",
    "             {\"https\": \"178.128.31.153:8080\"},\n",
    "             {\"http\": \"167.114.180.102:8080\"},\n",
    "             {\"http\": \"104.131.214.218:80\"},\n",
    "             {\"http\": \"167.114.196.153:80\"}]\n",
    "header = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example use of logmatic \n",
    "# more simple \n",
    "\n",
    "logger = logging.getLogger()\n",
    " \n",
    "handler = logging.FileHandler(get_time())\n",
    "handler.setFormatter(logmatic.JsonFormatter())\n",
    " \n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作 url List\n",
    "def get_url(n=2,m=3):\n",
    "    result = []\n",
    "    # 增加直覺 從第n頁 到 第m頁\n",
    "    m = m + 1\n",
    "    for t in range(n,m):\n",
    "        url = \"https://www.ptt.cc/bbs/Food/index\" + str(t) + \".html\"\n",
    "        result.append(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 連線流程\n",
    "def get_connect(url):\n",
    "    i = 0\n",
    "    resp = \"\"\n",
    "    try :\n",
    "        resp = requests.get(url,headers=header,proxies=proxylist[i])\n",
    "        resp.encoding = \"utf-8\"\n",
    "        time.sleep(random.randint(1,4))\n",
    "        # 設定連線狀況 log\n",
    "        code = \"HTTP response code = \" + str(resp.status_code)\n",
    "        logger.info(code,extra={\"url\" : url, \"time_use\" : resp.elapsed.total_seconds()})\n",
    "        while resp.status_code != 200 :\n",
    "            logger.info(\"Connect error\",extra={\"url\" : url, \"http response\": resp.raise_for_status()})\n",
    "            resp.close()\n",
    "            i = (i + 1) % 6\n",
    "            resp = requests.get(url,headers=header,proxies=proxylist[i])\n",
    "            resp.encoding = \"utf-8\"\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(e,exc_info=True)\n",
    "    \n",
    "    return resp.text \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(urls):\n",
    "# 進入ptt列表 抓取 標題、推文數、網址 \n",
    "    result = []\n",
    "    soup = BeautifulSoup(get_connect(urls))\n",
    "    print(\"進入ptt : \",urls)\n",
    "    post = soup.find_all(\"div\", class_=\"r-ent\")\n",
    "    for r in post:\n",
    "        # 取得推的數量\n",
    "        push = 0\n",
    "        if r.find(\"div\", class_=\"nrec\").text:\n",
    "            try:\n",
    "                push = int(r.find(\"div\", class_=\"nrec\").text)\n",
    "            except ValueError:\n",
    "                    pass\n",
    "        if r.find(\"a\"):\n",
    "            # 取得網址\n",
    "            href = \"http://www.ptt.cc\" + r.find(\"a\")[\"href\"]\n",
    "            # 取得標題\n",
    "            title = r.find(\"a\").text\n",
    "\n",
    "            # 進入第二層 進入每一篇\n",
    "            # 去除公告 請益 為分類的文章\n",
    "            if not \"公告\" in title :\n",
    "                if not \"請益\" in title:\n",
    "                    print(\"現在處理: \", href)\n",
    "                    art = BeautifulSoup(get_connect(href))\n",
    "                    article = []\n",
    "                    content = art.find(\"div\", id=\"main-content\")\n",
    "                    # 保存要丟棄的資訊\n",
    "                    val = content.find_all(\"span\", {\"class\":\"article-meta-value\"})\n",
    "                    #  文章排版問題 有些文章沒有部份以下資訊\n",
    "                    try:\n",
    "                        author = val[0].text\n",
    "                        title_in_content = val[2].text\n",
    "                        date_in_content = val[3].text\n",
    "                    except IndexError as e:\n",
    "                        logger.warning(\"article span list out of index\", extra = {\"url\" : href})\n",
    "                        pass\n",
    "\n",
    "                    # 開始丟棄資訊\n",
    "                    removes = content.find_all(\"div\", class_=\"article-metaline\")\n",
    "                    for remove in removes:\n",
    "                        remove.extract()\n",
    "                    removes = content.find_all(\"div\", class_=\"article-metaline-right\")\n",
    "                    for remove in removes:\n",
    "                        remove.extract()\n",
    "                    removes = content.find_all(\"span\", class_=\"f2\")\n",
    "                    for remove in removes:\n",
    "                        remove.extract()\n",
    "                    removes = content.find_all(\"div\", class_=\"push\")\n",
    "                    for remove in removes:\n",
    "                        remove.extract()\n",
    "                    article.append({\n",
    "                        \"author\" : author,\n",
    "                        \"title_in_content\" : title_in_content,\n",
    "                        \"date_in_content\" : date_in_content,\n",
    "                        \"content\" : content.text\n",
    "                    })\n",
    "\n",
    "        result.append({\n",
    "            \"title\" : title,\n",
    "            \"push\" : push,\n",
    "            \"href\" : href,\n",
    "            \"content\" : article\n",
    "\n",
    "        })\n",
    "        \n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "進入ptt :  https://www.ptt.cc/bbs/Food/index5.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1079547520.A.35E.html\n",
      "進入ptt :  https://www.ptt.cc/bbs/Food/index6.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1083830551.A.BA5.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1083844000.A.C1A.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1079567406.A.F5A.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1083986656.A.79E.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1079689943.A.9DE.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1084061525.A.AA2.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1079708431.A.BBA.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1079738475.A.15E.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1084230361.A.0B0.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1079866429.A.E5A.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1084231477.A.EAE.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1079876659.A.5CB.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1084232438.A.0E5.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1079987492.A.F01.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1084269729.A.76A.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1084581681.A.E5A.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1080077173.A.60F.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1080086454.A.43D.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1084588391.A.68C.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1080134207.A.EE2.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085135382.A.6E1.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1080141726.A.927.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085244828.A.A53.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1080218298.A.B62.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085342989.A.31A.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1080241344.A.6E1.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085509928.A.54C.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085533472.A.56F.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1080775988.A.5A8.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085615344.A.99B.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085702270.A.35E.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1081180900.A.BF3.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085712149.A.3A3.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1081279710.A.45C.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1085848022.A.4AE.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1082323893.A.299.html\n",
      "work has done\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1083237044.A.AF9.html\n",
      "現在處理:  http://www.ptt.cc/bbs/Food/M.1083635435.A.0AE.html\n",
      "work has done\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, wait, ProcessPoolExecutor, as_completed\n",
    "\n",
    "with ProcessPoolExecutor(max_workers= 2) as executor:\n",
    "    future_result = {executor.submit(parse, url) : url for url in get_url(5,6)}\n",
    "    \n",
    "    new_result = []\n",
    "    \n",
    "    for future in as_completed(future_result):\n",
    "        new_result.append(future.result())\n",
    "        print(\"work has done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  6 08:45:55 2004\n",
      "Thu May  6 11:55:55 2004\n",
      "Sat May  8 03:33:36 2004\n",
      "Sun May  9 00:42:56 2004\n",
      "Mon May 10 23:21:04 2004\n",
      "Mon May 10 23:32:42 2004\n",
      "Mon May 10 23:41:19 2004\n",
      "Tue May 11 10:08:44 2004\n",
      "Sat May 15 00:46:03 2004\n",
      "Sat May 15 02:50:29 2004\n",
      "Fri May 21 10:52:52 2004\n",
      "Sat May 22 16:56:04 2004\n",
      "Sun May 23 20:11:45 2004\n",
      "Tue May 25 18:32:07 2004\n",
      "Wed May 26 01:05:41 2004\n",
      "Wed May 26 23:53:39 2004\n",
      "Fri May 28 00:04:27 2004\n",
      "Fri May 28 03:07:18 2004\n",
      "Sat May 29 16:28:23 2004\n",
      "Sat May 29 16:28:23 2004\n",
      "Wed Mar 17 18:50:23 2004\n",
      "Thu Mar 18 00:10:14 2004\n",
      "Fri Mar 19 10:11:11 2004\n",
      "Fri Mar 19 15:09:56 2004\n",
      "Fri Mar 19 23:22:32 2004\n",
      "Sun Mar 21 10:55:54 2004\n",
      "Sun Mar 21 13:58:01 2004\n",
      "Mon Mar 22 20:43:59 2004\n",
      "Tue Mar 23 21:34:58 2004\n",
      "Wed Mar 24 00:05:10 2004\n",
      "Wed Mar 24 13:32:30 2004\n",
      "Wed Mar 24 15:22:05 2004\n",
      "Wed Mar 24 15:22:05 2004\n",
      "Thu Mar 25 19:11:48 2004\n",
      "Thu Apr  1 00:07:25 2004\n",
      "Mon Apr  5 16:27:22 2004\n",
      "Tue Apr  6 19:35:59 2004\n",
      "Sun Apr 18 22:26:57 2004\n",
      "Thu Apr 29 12:45:55 2004\n",
      "Tue May  4 02:05:56 2004\n"
     ]
    }
   ],
   "source": [
    "for r in new_result:\n",
    "    for er in r:\n",
    "        for author in er[\"content\"]:\n",
    "            print(author[\"date_in_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = parse('https://www.ptt.cc/bbs/Food/index5.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進入ptt列表 抓取 標題、推文數、網址 \n",
    "result = []\n",
    "soup = BeautifulSoup(get_connect(url))\n",
    "print(\"進入ptt : \",url)\n",
    "post = soup.find_all(\"div\", class_=\"r-ent\")\n",
    "for r in post:\n",
    "            # 取得推的數量\n",
    "    push = 0\n",
    "    if r.find(\"div\", class_=\"nrec\").text:\n",
    "        try:\n",
    "            push = int(r.find(\"div\", class_=\"nrec\").text)\n",
    "        except ValueError:\n",
    "                pass\n",
    "    if r.find(\"a\"):\n",
    "        # 取得網址\n",
    "        href = \"http://www.ptt.cc\" + r.find(\"a\")[\"href\"]\n",
    "        # 取得標題\n",
    "        title = r.find(\"a\").text\n",
    "        \n",
    "        # 進入第二層 進入每一篇\n",
    "        # 去除公告 請益 為分類的文章\n",
    "        if not \"公告\" in title :\n",
    "            if not \"請益\" in title:\n",
    "                print(\"現在處理: \", href)\n",
    "                art = BeautifulSoup(get_connect(href))\n",
    "                article = []\n",
    "                content = art.find(\"div\", id=\"main-content\")\n",
    "                # 保存要丟棄的資訊\n",
    "                val = content.find_all(\"span\", {\"class\":\"article-meta-value\"})\n",
    "                author = val[0].text\n",
    "                title_in_content = val[2].text\n",
    "                date_in_content = val[3].text\n",
    "                \n",
    "                # 開始丟棄資訊\n",
    "                removes = content.find_all(\"div\", class_=\"article-metaline\")\n",
    "                for remove in removes:\n",
    "                    remove.extract()\n",
    "                removes = content.find_all(\"div\", class_=\"article-metaline-right\")\n",
    "                for remove in removes:\n",
    "                    remove.extract()\n",
    "                removes = content.find_all(\"span\", class_=\"f2\")\n",
    "                for remove in removes:\n",
    "                    remove.extract()\n",
    "                removes = content.find_all(\"div\", class_=\"push\")\n",
    "                for remove in removes:\n",
    "                    remove.extract()\n",
    "                article.append({\n",
    "                    \"author\" : author,\n",
    "                    \"title_in_content\" : title_in_content,\n",
    "                    \"date_in_content\" : date_in_content,\n",
    "                    \"content\" : content.text\n",
    "                })\n",
    "                \n",
    "    result.append({\n",
    "        \"title\" : title,\n",
    "        \"push\" : push,\n",
    "        \"href\" : href,\n",
    "        \"content\" : article\n",
    "        \n",
    "    })\n",
    "    \n",
    "    time.sleep(random.randint(1,6))\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# 轉成 Dataframe\n",
    "df = pd.DataFrame(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 排列欄位順序\n",
    "\n",
    "df = df[ [\"title\", \"push\", \"href\", \"content\" ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O 較多 使用 thread\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = executor.map(int, ['1', '2', '3', '4', '5'])\n",
    "    for v in results:\n",
    "        print(v)\n",
    "end = time.time()\n",
    "print(\"time used :\",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example use of json_logging\n",
    "# more detail\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import json_logging\n",
    "\n",
    "# log is initialized without a web framework name\n",
    "json_logging.ENABLE_JSON_LOGGING = True\n",
    "json_logging.init()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "#logger.info(\"test log statement\")\n",
    "#logger.info(\"test log statement\", extra={'props': {\"extra_property\": 'extra_value'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging \n",
    "# 設定 log config \n",
    "# less detail\n",
    "\n",
    "handler = logging.FileHandler(get_time())  # Or FileHandler or anything else\n",
    "# Configure the fields to include in the JSON output. message is the main log string itself\n",
    "format_str = '%(message)%(levelname)%(name)%(asctime)'\n",
    "formatter = jsonlogger.JsonFormatter(format_str)\n",
    "handler.setFormatter(formatter)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "# Normally we would attach the handler to the root logger, and this would be unnecessary\n",
    "logger.propagate = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
